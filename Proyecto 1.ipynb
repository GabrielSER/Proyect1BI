{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YVaNOef-qDM"
   },
   "source": [
    "# Instalación e importanción de librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9nJ4vBJ-oJA",
    "outputId": "a9a3d17b-cf95-433b-da45-53ad8eb886c2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 8.0 MB/s \n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 39.0 MB/s \n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
      "Collecting scikit-plot\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.2)\n",
      "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.1.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.7)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n",
      "Installing collected packages: scikit-plot\n",
      "Successfully installed scikit-plot-0.3.7\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install plotly\n",
    "!pip install inflect\n",
    "!pip install scikit-plot\n",
    " # librería Natural Language Toolkit, usada para trabajar con textos \n",
    "import nltk\n",
    "# Punkt permite separar un texto en frases.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O16pQGapLTu5",
    "outputId": "6fdb0320-9d01-4529-d422-2a97cebd27b7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.68)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied: scikit-plot in /usr/local/lib/python3.7/dist-packages (0.3.7)\n",
      "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.2)\n",
      "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "#Esta installación es opcional. En Mac no me servia instalarlo de la forma de arriba\n",
    "import sys\n",
    "!{sys.executable} -m pip install contractions\n",
    "!{sys.executable} -m pip install inflect\n",
    "!{sys.executable} -m pip install plotly\n",
    "!{sys.executable} -m pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "S7Q_BimpDZKT"
   },
   "outputs": [],
   "source": [
    "# Instalación de librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(3301)\n",
    "import sys\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_precision_recall_curve,accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import scikitplot.metrics as skplt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import dropwhile\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import reader,writer\n",
    "import operator as op\n",
    "import string\n",
    "from sklearn import neighbors\n",
    "\n",
    "# Para preparar los datos\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Para crear el arbol de decisión \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "# Para usar KNN como clasificador\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Para realizar la separación del conjunto de aprendizaje en entrenamiento y test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Para evaluar el modelo\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# Para búsqueda de hiperparámetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Para la validación cruzada\n",
    "from sklearn.model_selection import KFold \n",
    "#Librerías para la visualización\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhuNXsgBVey"
   },
   "source": [
    "# 2 Perfilamiento y entendimiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VfS42Gcj_vsg"
   },
   "outputs": [],
   "source": [
    "# Uso de la libreria pandas para la lectura de archivos\n",
    "df=pd.read_csv('clinical_trials_on_cancer_data_clasificacion.csv', sep=',', encoding = 'utf-8')\n",
    "# Asignación a una nueva variable de los datos leidos\n",
    "df_t=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "z7BzhrZE_vt2",
    "outputId": "c10476a1-9073-45b1-9d11-0b4bcb21d1ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>study_and_condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Saracatinib . recurren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Stem cell transplantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Lenograstim . recurren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                study_and_condition\n",
       "0  __label__0  study interventions are Saracatinib . recurren...\n",
       "1  __label__1  study interventions are Stem cell transplantat...\n",
       "2  __label__0  study interventions are Lenograstim . recurren...\n",
       "3  __label__0  study interventions are Doxorubicin . stage ii...\n",
       "4  __label__1  study interventions are Poly I-C . prostate ca..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "b9X04Ve4_vvb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>study_and_condition</th>\n",
       "      <th>Conteo</th>\n",
       "      <th>Moda</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Saracatinib . recurren...</td>\n",
       "      <td>250</td>\n",
       "      <td>of</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Stem cell transplantat...</td>\n",
       "      <td>224</td>\n",
       "      <td>the</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Lenograstim . recurren...</td>\n",
       "      <td>229</td>\n",
       "      <td>performed</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
       "      <td>268</td>\n",
       "      <td>stage</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
       "      <td>232</td>\n",
       "      <td>iraes</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                study_and_condition  Conteo  \\\n",
       "0  __label__0  study interventions are Saracatinib . recurren...     250   \n",
       "1  __label__1  study interventions are Stem cell transplantat...     224   \n",
       "2  __label__0  study interventions are Lenograstim . recurren...     229   \n",
       "3  __label__0  study interventions are Doxorubicin . stage ii...     268   \n",
       "4  __label__1  study interventions are Poly I-C . prostate ca...     232   \n",
       "\n",
       "        Moda  Max  Min  \n",
       "0         of   15    1  \n",
       "1        the   15    1  \n",
       "2  performed   13    1  \n",
       "3      stage   13    1  \n",
       "4      iraes   15    1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textos = df_t.copy()\n",
    "textos['Conteo'] = [len(x) for x in textos['study_and_condition']]\n",
    "textos['Moda'] = [max(set(x.split(' ')), key = x.split(' ').count) for x in textos['study_and_condition']]\n",
    "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['study_and_condition']]\n",
    "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['study_and_condition']]\n",
    "textos.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGNg-C8XEl5G"
   },
   "source": [
    "# 3 Preparación y limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kgwiFTCXEjEL"
   },
   "outputs": [],
   "source": [
    "def uselessdata(words):\n",
    "    dot = words.index('.')\n",
    "    new_words = words[dot+1:]\n",
    "    return new_words\n",
    "\n",
    "def nonascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def removestopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = uselessdata(words)\n",
    "    words = lowercase(words)\n",
    "    words = numbers(words)\n",
    "    words = punctuation(words)\n",
    "    words = nonascii(words)\n",
    "    words = removestopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "E0rZlaC4FMnN"
   },
   "outputs": [],
   "source": [
    "df_t['study_and_condition'] = df_t['study_and_condition'].apply(contractions.fix) #Aplica la corrección de las contracciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wWovYSBGFd5n",
    "outputId": "f919cb43-8712-4123-8bd3-bb18ea8ca883"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>study_and_condition</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Saracatinib . recurren...</td>\n",
       "      <td>[recurrent, verrucous, carcinoma, larynx, diag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Stem cell transplantat...</td>\n",
       "      <td>[hodgkin, lymphoma, diagnosis, history, congen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Lenograstim . recurren...</td>\n",
       "      <td>[recurrent, adult, diffuse, mixed, cell, lymph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
       "      <td>[stage, iii, diffuse, large, cell, lymphoma, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
       "      <td>[prostate, cancer, diagnosis, unresolved, irae...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                study_and_condition  \\\n",
       "0  __label__0  study interventions are Saracatinib . recurren...   \n",
       "1  __label__1  study interventions are Stem cell transplantat...   \n",
       "2  __label__0  study interventions are Lenograstim . recurren...   \n",
       "3  __label__0  study interventions are Doxorubicin . stage ii...   \n",
       "4  __label__1  study interventions are Poly I-C . prostate ca...   \n",
       "\n",
       "                                               words  \n",
       "0  [recurrent, verrucous, carcinoma, larynx, diag...  \n",
       "1  [hodgkin, lymphoma, diagnosis, history, congen...  \n",
       "2  [recurrent, adult, diffuse, mixed, cell, lymph...  \n",
       "3  [stage, iii, diffuse, large, cell, lymphoma, d...  \n",
       "4  [prostate, cancer, diagnosis, unresolved, irae...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t['words'] = df_t['study_and_condition'].apply(word_tokenize).apply(preprocessing) #Aplica la eliminación del ruido\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8FBl5157F5MA",
    "outputId": "81ac29bd-004a-4109-ddaf-8127de39d841"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>study_and_condition</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Saracatinib . recurren...</td>\n",
       "      <td>[recur, verruc, carcinom, larynx, diagnos, pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Stem cell transplantat...</td>\n",
       "      <td>[hodgkin, lymphom, diagnos, hist, congenit, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Lenograstim . recurren...</td>\n",
       "      <td>[recur, adult, diffus, mix, cel, lymphom, diag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
       "      <td>[stag, ii, diffus, larg, cel, lymphom, diagnos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
       "      <td>[prost, cant, diagnos, unresolv, ira, follow, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                study_and_condition  \\\n",
       "0  __label__0  study interventions are Saracatinib . recurren...   \n",
       "1  __label__1  study interventions are Stem cell transplantat...   \n",
       "2  __label__0  study interventions are Lenograstim . recurren...   \n",
       "3  __label__0  study interventions are Doxorubicin . stage ii...   \n",
       "4  __label__1  study interventions are Poly I-C . prostate ca...   \n",
       "\n",
       "                                               words  \n",
       "0  [recur, verruc, carcinom, larynx, diagnos, pat...  \n",
       "1  [hodgkin, lymphom, diagnos, hist, congenit, he...  \n",
       "2  [recur, adult, diffus, mix, cel, lymphom, diag...  \n",
       "3  [stag, ii, diffus, larg, cel, lymphom, diagnos...  \n",
       "4  [prost, cant, diagnos, unresolv, ira, follow, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemw(palabras):\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for palabra in palabras:\n",
    "        stem = stemmer.stem(palabra)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatizew(palabras):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for palabra in palabras:\n",
    "        lemma = lemmatizer.lemmatize(palabra, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stemlemmatize(palabras):\n",
    "    stems = stemw(palabras)\n",
    "    lemmas = lemmatizew(palabras)\n",
    "    return stems + lemmas\n",
    "\n",
    "df_t['words'] = df_t['words'].apply(stemlemmatize) \n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "gc8Ea3M_LTvP",
    "outputId": "033c8a0a-60db-4df9-b060-cbfdbaa54e12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0   6000\n",
       "1   6000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se arreglan datos puntuales\n",
    "atributo = \"label\"\n",
    "df_t[atributo].replace(\"__label__0\", 0, inplace = True)\n",
    "df_t[atributo].value_counts().to_frame()\n",
    "df_t[atributo].replace(\"__label__1\", 1, inplace = True)\n",
    "df_t[atributo].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "JUKv4SwyGVIB",
    "outputId": "20d20322-8986-4430-a7b4-d208a2ea0bab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>study_and_condition</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Saracatinib . recurren...</td>\n",
       "      <td>recur verruc carcinom larynx diagnos paty must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Stem cell transplantat...</td>\n",
       "      <td>hodgkin lymphom diagnos hist congenit hematolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Lenograstim . recurren...</td>\n",
       "      <td>recur adult diffus mix cel lymphom diagnos cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
       "      <td>stag ii diffus larg cel lymphom diagnos stag i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
       "      <td>prost cant diagnos unresolv ira follow pri bio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Prednisolone hemisucci...</td>\n",
       "      <td>recur child larg cel lymphom diagnos known hyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Bevacizumab . recurren...</td>\n",
       "      <td>recur rect cant diagnos absolv neutrophil coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Antibodies, Monoclonal...</td>\n",
       "      <td>recur lymphoblast lymphom diagnos intrathec in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Vorinostat . colorecta...</td>\n",
       "      <td>colorect cant diagnos paty must receiv least o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Freund's Adjuvant . ov...</td>\n",
       "      <td>ov cant diagnos four week sint pri particip in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                study_and_condition  \\\n",
       "0          0  study interventions are Saracatinib . recurren...   \n",
       "1          1  study interventions are Stem cell transplantat...   \n",
       "2          0  study interventions are Lenograstim . recurren...   \n",
       "3          0  study interventions are Doxorubicin . stage ii...   \n",
       "4          1  study interventions are Poly I-C . prostate ca...   \n",
       "...      ...                                                ...   \n",
       "11995      0  study interventions are Prednisolone hemisucci...   \n",
       "11996      0  study interventions are Bevacizumab . recurren...   \n",
       "11997      1  study interventions are Antibodies, Monoclonal...   \n",
       "11998      0  study interventions are Vorinostat . colorecta...   \n",
       "11999      0  study interventions are Freund's Adjuvant . ov...   \n",
       "\n",
       "                                                   words  \n",
       "0      recur verruc carcinom larynx diagnos paty must...  \n",
       "1      hodgkin lymphom diagnos hist congenit hematolo...  \n",
       "2      recur adult diffus mix cel lymphom diagnos cre...  \n",
       "3      stag ii diffus larg cel lymphom diagnos stag i...  \n",
       "4      prost cant diagnos unresolv ira follow pri bio...  \n",
       "...                                                  ...  \n",
       "11995  recur child larg cel lymphom diagnos known hyp...  \n",
       "11996  recur rect cant diagnos absolv neutrophil coun...  \n",
       "11997  recur lymphoblast lymphom diagnos intrathec in...  \n",
       "11998  colorect cant diagnos paty must receiv least o...  \n",
       "11999  ov cant diagnos four week sint pri particip in...  \n",
       "\n",
       "[12000 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No se debe correr este para KNN\n",
    "df_t['words'] = df_t['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yr8x854PG7Wb"
   },
   "outputs": [],
   "source": [
    "filename = 'processed_data.csv'\n",
    "df_t.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoQ-HAbuJGcp"
   },
   "source": [
    "# 4.Creación y aplicación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56cvArsbJeRe"
   },
   "source": [
    "### 4.1 Regresión Logística\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "Nhe9xjEOU2LL"
   },
   "outputs": [],
   "source": [
    "data_rl = df_t.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_rl['words'], data_rl['label'], test_size = 0.2, random_state= 99)\n",
    "cv = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "X_train =  cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "A1ARAqT7X_5e",
    "outputId": "82fa1203-57a4-4762-bac5-39ad3ff3d532"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = LogisticRegression()\n",
    "rl.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "g9mWDm6dYCN_"
   },
   "outputs": [],
   "source": [
    "predictions = rl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "FG1z7uLRYEJw",
    "outputId": "e1760b12-e486-4457-f3d5-841aa4dfab6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy = 79.38%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(predictions, y_test)\n",
    "print(f\"Model Accuracy = {round(acc*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "DhprLVMMYI0j",
    "outputId": "c941d567-d2d2-4a20-feec-0c56ec659e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      " [[948 276]\n",
      " [219 957]]\n"
     ]
    }
   ],
   "source": [
    "new = np.asarray(y_test)\n",
    "print('Confusion Matrix:\\n\\n', confusion_matrix(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "MLfwxGZOYJfY",
    "outputId": "c5e8db46-9c7e-427c-d927-e86a859d7f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      1224\n",
      "           1       0.78      0.81      0.79      1176\n",
      "\n",
      "    accuracy                           0.79      2400\n",
      "   macro avg       0.79      0.79      0.79      2400\n",
      "weighted avg       0.79      0.79      0.79      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:\\n\\n', classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "Mad-rVw0YN0I",
    "outputId": "6bcce1f9-b126-4067-e974-ecc04c77ee1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion Matrix'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coso = skplt.plot_confusion_matrix(y_test,predictions)\n",
    "coso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lCgv59FJklZ"
   },
   "source": [
    "### 4.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "E0ax2cZMIwRL"
   },
   "outputs": [],
   "source": [
    "df_nb = df_t.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "QLBKvC9HJNmd"
   },
   "outputs": [],
   "source": [
    "def Text_Into_Vector(model,data):\n",
    "    model_vect = model(ngram_range=(1,2)) #in scikit-learn\n",
    "    final_array = model_vect.fit_transform(data.values)\n",
    "    \n",
    "    return model_vect, final_array\n",
    "\n",
    "def Split_data(x_vec, y_vec):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_vec, y_vec, test_size=.33, random_state=0)\n",
    "    xt, xc, yt, yc = train_test_split(X_train, Y_train, test_size=.33, random_state=0)\n",
    "    return xt, xc, X_test, yt, Y_test, yc, X_train, Y_train\n",
    "\n",
    "def Normalization(train, cv, test):\n",
    "    train=preprocessing.normalize(train)\n",
    "    cv=preprocessing.normalize(cv)\n",
    "    test=preprocessing.normalize(test)\n",
    "    \n",
    "    return train, cv, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "h05q0BPuJSJE"
   },
   "outputs": [],
   "source": [
    "def Multinomial_NB(X_train,xc,Y_train,yc):\n",
    "    best_alpha=0\n",
    "    max_roc_auc=-1\n",
    "    pred_cv = []\n",
    "    pred_train = []\n",
    "    alpha=[10000,5000,1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001,0.00005,0.00001]\n",
    "\n",
    "    for i in alpha:\n",
    "        mulbnb = MultinomialNB(alpha=i)\n",
    "        mulbnb.fit(X_train,Y_train)\n",
    "        probs = mulbnb.predict_proba(xc)[:,1]     \n",
    "        prob = mulbnb.predict_proba(X_train)[:,1]\n",
    "\n",
    "        auc_score_cv = roc_auc_score(yc,probs)\n",
    "        auc_score_train = roc_auc_score(Y_train,prob)\n",
    "\n",
    "        pred_cv.append(auc_score_cv)\n",
    "        pred_train.append(auc_score_train)\n",
    "\n",
    "        if(max_roc_auc<auc_score_cv):\n",
    "            max_roc_auc=auc_score_cv\n",
    "            best_alpha=i\n",
    "    return best_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "nBDQD9whJdSc"
   },
   "outputs": [],
   "source": [
    "def Testing_model(X_train,Y_train,X_test,Y_test,best_alpha):\n",
    "    \n",
    "    bnb = MultinomialNB(alpha = best_alpha, fit_prior=True, class_prior=None)\n",
    "    bnb.fit(X_train,Y_train)\n",
    "    probs = bnb.predict_proba(X_test)[:,1]\n",
    "\n",
    "    roc_auc = roc_auc_score(Y_test,probs)\n",
    "\n",
    "    prediction=bnb.predict(X_test)\n",
    "    skplt.plot_confusion_matrix(Y_test,prediction)\n",
    "\n",
    "    print(\"macro f1 score for data :\",metrics.f1_score(Y_test, prediction, average = 'macro'))\n",
    "    print(\"micro f1 score for data:\",metrics.f1_score(Y_test, prediction, average = 'micro'))\n",
    "    print(\"hamming loss for data:\",metrics.hamming_loss(Y_test,prediction))\n",
    "    print(\"\\n\")\n",
    "    print(\"Precision recall report for data:\\n\",metrics.classification_report(Y_test, prediction))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return bnb,roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "fH8AADqyJgoK",
    "outputId": "b22d66c9-252e-4d64-892f-0150198d7fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score for data : 0.8014809205954296\n",
      "micro f1 score for data: 0.8015151515151515\n",
      "hamming loss for data: 0.1984848484848485\n",
      "\n",
      "\n",
      "Precision recall report for data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80      1995\n",
      "           1       0.80      0.79      0.80      1965\n",
      "\n",
      "    accuracy                           0.80      3960\n",
      "   macro avg       0.80      0.80      0.80      3960\n",
      "weighted avg       0.80      0.80      0.80      3960\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BOW, X = Text_Into_Vector(CountVectorizer,df_nb['words'])\n",
    "xt, xc, X_test, yt, Y_test, yc, X_train, Y_train = Split_data(X, df_nb['label'])\n",
    "best_alpha_bow = Multinomial_NB(xt,xc,yt,yc)\n",
    "NB_bow, roc_auc_bow = Testing_model(xt,yt,X_test,Y_test,best_alpha_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMkoXhs_LTvj"
   },
   "source": [
    "### 4.3 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "83gSHgzeLTvk",
    "outputId": "869be8ef-3c78-4ab0-dbce-b74f57c362fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>study_and_condition</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Saracatinib . recurren...</td>\n",
       "      <td>[recur, verruc, carcinom, larynx, diagnos, pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Stem cell transplantat...</td>\n",
       "      <td>[hodgkin, lymphom, diagnos, hist, congenit, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Lenograstim . recurren...</td>\n",
       "      <td>[recur, adult, diffus, mix, cel, lymphom, diag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
       "      <td>[stag, ii, diffus, larg, cel, lymphom, diagnos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
       "      <td>[prost, cant, diagnos, unresolv, ira, follow, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                study_and_condition  \\\n",
       "0      0  study interventions are Saracatinib . recurren...   \n",
       "1      1  study interventions are Stem cell transplantat...   \n",
       "2      0  study interventions are Lenograstim . recurren...   \n",
       "3      0  study interventions are Doxorubicin . stage ii...   \n",
       "4      1  study interventions are Poly I-C . prostate ca...   \n",
       "\n",
       "                                               words  \n",
       "0  [recur, verruc, carcinom, larynx, diagnos, pat...  \n",
       "1  [hodgkin, lymphom, diagnos, hist, congenit, he...  \n",
       "2  [recur, adult, diffus, mix, cel, lymphom, diag...  \n",
       "3  [stag, ii, diffus, larg, cel, lymphom, diagnos...  \n",
       "4  [prost, cant, diagnos, unresolv, ira, follow, ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_k = df_t.copy()\n",
    "df_k.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1fYwlVHjS1s"
   },
   "source": [
    "#### **Eliminación del Ruido**\n",
    "La eliminación del ruido se utiliza para dejar el archivo en texto plano. También para eliminar caracteres especiales y pasar todo a minúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "JpSYFvTcNVS9"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words, stopwords=stopwords.words('english')):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def preproccesing(words):\n",
    "    words = to_lowercase(words)\n",
    "    #words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvgGfctljbg3"
   },
   "source": [
    "#### **Tokenización**\n",
    "La tokenización permite dividir frases u oraciones en palabras. Con el fin de desglozar las palabras correctamente para el posterior análisis. Pero primero, se realiza una corrección de las contracciones que pueden estar presentes en los textos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKkoUxB_NOkf",
    "outputId": "48c2bf0d-7d20-449e-e142-fca11e3a259a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [study, interventions, saracatinib, recurrent,...\n",
       "1    [study, interventions, stem, cell, transplanta...\n",
       "2    [study, interventions, lenograstim, recurrent,...\n",
       "3    [study, interventions, doxorubicin, stage, iii...\n",
       "4    [study, interventions, poly, ic, prostate, can...\n",
       "Name: study_and_condition, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train= df_k['study_and_condition'] .apply(contractions.fix) #Aplica la corrección de las contracciones\n",
    "new_X_train = new_X_train.apply(word_tokenize)\n",
    "new_X_train = new_X_train.apply(preproccesing) #Aplica la eliminación del ruido\n",
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gpij5N4pjjuB"
   },
   "source": [
    "#### **Normalización**\n",
    "Para la normalización de los datos se realiza una eliminación de prefijos y sufijos, además de realizar una lemmatización de los verbos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRTkKXXuO7Ny",
    "outputId": "393ead6c-af8f-45a8-8fd2-566d8d9c751a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [study, interv, saracatinib, recur, verruc, ca...\n",
       "1    [study, interv, stem, cel, transpl, hodgkin, l...\n",
       "2    [study, interv, lenograstim, recur, adult, dif...\n",
       "3    [study, interv, doxorubicin, stag, ii, diffus,...\n",
       "4    [study, interv, poly, ic, prost, cant, diagnos...\n",
       "Name: study_and_condition, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    words = stem_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "new_X_train = new_X_train.apply(stem_and_lemmatize) #Aplica lematización y Eliminación de Prefijos y Sufijos.\n",
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VgGFV2cj3Wh"
   },
   "source": [
    "#### **Transformación de campos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "eT0vQ0-oPBpf",
    "outputId": "b15309c2-f3b7-4298-d8c9-12ec2b63da6e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>study_and_condition</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Saracatinib . recurren...</td>\n",
       "      <td>study interv saracatinib recur verruc carcinom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Stem cell transplantat...</td>\n",
       "      <td>study interv stem cel transpl hodgkin lymphom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Lenograstim . recurren...</td>\n",
       "      <td>study interv lenograstim recur adult diffus mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
       "      <td>study interv doxorubicin stag ii diffus larg c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
       "      <td>study interv poly ic prost cant diagnos unreso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Prednisolone hemisucci...</td>\n",
       "      <td>study interv prednisolon hemisuccin recur chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Bevacizumab . recurren...</td>\n",
       "      <td>study interv bevacizumab recur rect cant diagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>1</td>\n",
       "      <td>study interventions are Antibodies, Monoclonal...</td>\n",
       "      <td>study interv antibody monoclon recur lymphobla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Vorinostat . colorecta...</td>\n",
       "      <td>study interv vorinost colorect cant diagnos pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>0</td>\n",
       "      <td>study interventions are Freund's Adjuvant . ov...</td>\n",
       "      <td>study interv freund adjuv ov cant diagnos four...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                study_and_condition  \\\n",
       "0          0  study interventions are Saracatinib . recurren...   \n",
       "1          1  study interventions are Stem cell transplantat...   \n",
       "2          0  study interventions are Lenograstim . recurren...   \n",
       "3          0  study interventions are Doxorubicin . stage ii...   \n",
       "4          1  study interventions are Poly I-C . prostate ca...   \n",
       "...      ...                                                ...   \n",
       "11995      0  study interventions are Prednisolone hemisucci...   \n",
       "11996      0  study interventions are Bevacizumab . recurren...   \n",
       "11997      1  study interventions are Antibodies, Monoclonal...   \n",
       "11998      0  study interventions are Vorinostat . colorecta...   \n",
       "11999      0  study interventions are Freund's Adjuvant . ov...   \n",
       "\n",
       "                                                   words  \n",
       "0      study interv saracatinib recur verruc carcinom...  \n",
       "1      study interv stem cel transpl hodgkin lymphom ...  \n",
       "2      study interv lenograstim recur adult diffus mi...  \n",
       "3      study interv doxorubicin stag ii diffus larg c...  \n",
       "4      study interv poly ic prost cant diagnos unreso...  \n",
       "...                                                  ...  \n",
       "11995  study interv prednisolon hemisuccin recur chil...  \n",
       "11996  study interv bevacizumab recur rect cant diagn...  \n",
       "11997  study interv antibody monoclon recur lymphobla...  \n",
       "11998  study interv vorinost colorect cant diagnos pa...  \n",
       "11999  study interv freund adjuv ov cant diagnos four...  \n",
       "\n",
       "[12000 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_k['words'] = new_X_train.apply(lambda x: ' '.join(map(str, x)))\n",
    "df_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyg0ovQSj9Ap"
   },
   "source": [
    "Realizar la transformación Term-frecuency times inverse Document-frecuency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBfTI-z6PaeG",
    "outputId": "b35c4185-73df-459b-d1e5-98ee84d8142f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 6279)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "X_tf_idf = tf_idf.fit_transform(df_k['words'])\n",
    "print(X_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDiyZ4BSe6Hb",
    "outputId": "152552ed-1102-4212-b39c-62216f0aa187"
   },
   "outputs": [],
   "source": [
    "response = tf_idf.fit_transform(df_k['words'])\n",
    "classes = df_k['label']\n",
    "feature_names = tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWZOgQKAkEr1"
   },
   "source": [
    "### **Modelado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBKcEvqegG4S",
    "outputId": "c8c8b654-064a-49dc-aaa8-a06e90bc3aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud sobre entrenamiento: 0.86\n",
      "Exactitud sobre test: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en entrenamiento y test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(response, classes, test_size=0.2, random_state=0)\n",
    "\n",
    "# Utilicemos un número de vecinos = 2 \n",
    "modelo_knn = KNeighborsClassifier(n_neighbors=2)\n",
    "modelo_knn = modelo_knn.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_train = modelo_knn.predict(X_train)\n",
    "y_pred_test = modelo_knn.predict(X_test)\n",
    "print('Exactitud sobre entrenamiento: %.2f' % accuracy_score(Y_train, y_pred_train))\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "r3OGY5JRi1QR",
    "outputId": "43d45147-7c86-4258-bf44-c1862c7b5031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77      1228\n",
      "           1       0.83      0.54      0.66      1172\n",
      "\n",
      "    accuracy                           0.72      2400\n",
      "   macro avg       0.75      0.72      0.71      2400\n",
      "weighted avg       0.75      0.72      0.71      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/h1p1q28j2gvf57nvm7lfjxfw0000gn/T/ipykernel_90901/283544654.py:5: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Mostrar reporte de clasificación sobre test\n",
    "print(classification_report(Y_test, y_pred_test))\n",
    "# visualizar la matriz de confusión \n",
    "plot_confusion_matrix(modelo_knn, X_test, Y_test)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqtoBCV3ip90"
   },
   "source": [
    "#### Búsqueda con hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "IMab-NPJgY1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'p': [1, 2]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero definamos el espacio de búsqueda\n",
    "n_vecinos =  list(range(1,11))\n",
    "# Fijemos el número de particiones. Utilizaremos K = 10.\n",
    "particiones = KFold(n_splits=10, shuffle=True, random_state = 0)\n",
    "\n",
    "param_grid = {'n_neighbors': n_vecinos, 'p': [1, 2]} #1 Manhattan, 2 euclidiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Q2vYTJ4hD7K",
    "outputId": "bafa5ccd-4ee7-4f7a-d8fc-4af9278101bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor parámetro: {'n_neighbors': 5, 'p': 2}\n",
      "Mejor cross-validation score: 0.76\n",
      "CPU times: user 5min 9s, sys: 11.7 s, total: 5min 20s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clasificadorKNN = KNeighborsClassifier()\n",
    "modelo_Knn = GridSearchCV(clasificadorKNN, param_grid, cv=particiones)\n",
    "modelo_Knn.fit(X_train,Y_train) \n",
    "print(\"Mejor parámetro: {}\".format(modelo_Knn.best_params_)) \n",
    "print(\"Mejor cross-validation score: {:.2f}\".format(modelo_Knn.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DY5iMolchKCv",
    "outputId": "d0c032d9-b04d-4946-db24-c0e525b88eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud sobre entrenamiento: 0.84\n",
      "Exactitud sobre test: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Obtener el mejor modelo.\n",
    "modelo_final = modelo_Knn.best_estimator_\n",
    "# Probemos ahora este modelo sobre test.\n",
    "y_pred_train = modelo_final.predict(X_train)\n",
    "y_pred_test = modelo_final.predict(X_test)\n",
    "print('Exactitud sobre entrenamiento: %.2f' % accuracy_score(Y_train, y_pred_train))\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "Xx_vb06OhONQ",
    "outputId": "9b3f56df-d3a2-41d8-a840-77e182b7c2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78      1228\n",
      "           1       0.78      0.73      0.76      1172\n",
      "\n",
      "    accuracy                           0.77      2400\n",
      "   macro avg       0.77      0.77      0.77      2400\n",
      "weighted avg       0.77      0.77      0.77      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/h1p1q28j2gvf57nvm7lfjxfw0000gn/T/ipykernel_90901/1854221540.py:5: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Mostrar reporte de clasificación sobre test\n",
    "print(classification_report(Y_test, y_pred_test))\n",
    "# visualizar la matriz de confusión \n",
    "plot_confusion_matrix(modelo_Knn, X_test, Y_test)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh3wqA5Gm5UP"
   },
   "source": [
    "#### Normalizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "vwMPK14Lm80_"
   },
   "outputs": [],
   "source": [
    "#Normalizar los datos\n",
    "norm=MaxAbsScaler()\n",
    "X_train_n=norm.fit_transform(X_train)\n",
    "X_test_n=norm.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "oR05fJ-lnEiK"
   },
   "outputs": [],
   "source": [
    "# Primero definamos el espacio de búsqueda\n",
    "n_vecinos =  list(range(1,11))\n",
    "# Fijemos el número de particiones. Utilizaremos K = 10.\n",
    "particiones = KFold(n_splits=10, shuffle=True, random_state = 0)\n",
    "\n",
    "param_grid = {'n_neighbors': n_vecinos, 'p': [1, 2]} #1 Manhattan, 2 euclidiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZspQMjvnJyj",
    "outputId": "4c6548a6-25d1-453f-99aa-e38b574f23ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor parámetro: {'n_neighbors': 1, 'p': 2}\n",
      "Mejor cross-validation score: 0.73\n",
      "CPU times: user 4min 59s, sys: 12.5 s, total: 5min 12s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clasificadorKNN = KNeighborsClassifier()\n",
    "modelo_Knn = GridSearchCV(clasificadorKNN, param_grid, cv=particiones)\n",
    "modelo_Knn.fit(X_train_n,Y_train) \n",
    "print(\"Mejor parámetro: {}\".format(modelo_Knn.best_params_)) \n",
    "print(\"Mejor cross-validation score: {:.2f}\".format(modelo_Knn.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sY1XtBOYnPSY",
    "outputId": "a1e363e3-f064-4799-cefe-196b483a7ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud sobre entrenamiento: 1.00\n",
      "Exactitud sobre test: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Obtener el mejor modelo.\n",
    "modelo_final = modelo_Knn.best_estimator_\n",
    "# Probemos ahora este modelo sobre test.\n",
    "y_pred_train = modelo_final.predict(X_train_n)\n",
    "y_pred_test = modelo_final.predict(X_test_n)\n",
    "print('Exactitud sobre entrenamiento: %.2f' % accuracy_score(Y_train, y_pred_train))\n",
    "print('Exactitud sobre test: %.2f' % accuracy_score(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "DjRc86xNnSwJ",
    "outputId": "50b3d31b-e006-4934-f0dc-67b02a3f8f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76      1228\n",
      "           1       0.76      0.69      0.72      1172\n",
      "\n",
      "    accuracy                           0.74      2400\n",
      "   macro avg       0.74      0.74      0.74      2400\n",
      "weighted avg       0.74      0.74      0.74      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/h1p1q28j2gvf57nvm7lfjxfw0000gn/T/ipykernel_90901/2234322628.py:5: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Mostrar reporte de clasificación sobre test\n",
    "print(classification_report(Y_test, y_pred_test))\n",
    "# visualizar la matriz de confusión \n",
    "plot_confusion_matrix(modelo_Knn, X_test_n, Y_test)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-wHI3pkfgID",
    "outputId": "b1df38c8-3da3-4b5e-e67a-9732cd7ef113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsRegressor(n_neighbors=2)\n",
    "clf.fit(response, classes)\n",
    "clf.predict(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "56cvArsbJeRe",
    "-lCgv59FJklZ"
   ],
   "name": "Proyecto 1 knn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
